{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae29a88-0c15-401d-8107-b41a0aac5450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting langchain\n",
      "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.35-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.10.8-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n",
      "  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.13.1-cp311-cp311-win_amd64.whl.metadata (52 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.7-cp311-none-win_amd64.whl.metadata (51 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.23.4-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.8/1.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.10.8-cp311-cp311-win_amd64.whl (381 kB)\n",
      "Downloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n",
      "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/15.8 MB 2.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.3/15.8 MB 2.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.1/15.8 MB 2.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.6/15.8 MB 2.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 3.1/15.8 MB 2.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.7/15.8 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.5/15.8 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.7/15.8 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.0/15.8 MB 2.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.8/15.8 MB 2.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.3/15.8 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 7.1/15.8 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 7.6/15.8 MB 2.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 8.4/15.8 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.9/15.8 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.2/15.8 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.0/15.8 MB 2.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 10.5/15.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.3/15.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.1/15.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.6/15.8 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.4/15.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.8 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.35-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Downloading greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.7-cp311-none-win_amd64.whl (137 kB)\n",
      "Downloading yarl-1.13.1-cp311-cp311-win_amd64.whl (111 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: tenacity, pydantic-core, orjson, numpy, multidict, jsonpointer, greenlet, frozenlist, annotated-types, aiohappyeyeballs, yarl, SQLAlchemy, pydantic, jsonpatch, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.2 aiohttp-3.10.8 aiosignal-1.3.1 annotated-types-0.7.0 frozenlist-1.4.1 greenlet-3.1.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-core-0.3.6 langchain-text-splitters-0.3.0 langsmith-0.1.129 multidict-6.1.0 numpy-1.26.4 orjson-3.10.7 pydantic-2.9.2 pydantic-core-2.23.4 tenacity-8.5.0 yarl-1.13.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires filelock, which is not installed.\n",
      "datasets 2.20.0 requires huggingface-hub>=0.21.2, which is not installed.\n",
      "datasets 2.20.0 requires pandas, which is not installed.\n",
      "datasets 2.20.0 requires pyarrow>=15.0.0, which is not installed.\n",
      "datasets 2.20.0 requires pyarrow-hotfix, which is not installed.\n",
      "datasets 2.20.0 requires tqdm>=4.66.3, which is not installed.\n",
      "datasets 2.20.0 requires xxhash, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5743df-37a0-4041-9b70-f678aa5e93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-openai) (0.3.6)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
      "  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (0.1.129)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.2.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Downloading jiter-0.5.0-cp311-none-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.0)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-openai) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.40.0->langchain-openai) (0.4.6)\n",
      "Downloading langchain_openai-0.2.1-py3-none-any.whl (49 kB)\n",
      "Downloading openai-1.50.2-py3-none-any.whl (382 kB)\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.0 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 524.3/799.0 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 799.0/799.0 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.5.0-cp311-none-win_amd64.whl (191 kB)\n",
      "Downloading regex-2024.9.11-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, jiter, distro, tiktoken, openai, langchain-openai\n",
      "Successfully installed distro-1.9.0 jiter-0.5.0 langchain-openai-0.2.1 openai-1.50.2 regex-2024.9.11 tiktoken-0.7.0 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires filelock, which is not installed.\n",
      "datasets 2.20.0 requires huggingface-hub>=0.21.2, which is not installed.\n",
      "datasets 2.20.0 requires pandas, which is not installed.\n",
      "datasets 2.20.0 requires pyarrow>=15.0.0, which is not installed.\n",
      "datasets 2.20.0 requires pyarrow-hotfix, which is not installed.\n",
      "datasets 2.20.0 requires xxhash, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d1e8fe-3f9a-4c18-972d-d51cdcbe91de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-communityNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (3.10.8)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (0.3.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (0.1.129)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.13.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain<0.4.0,>=0.3.1->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain<0.4.0,>=0.3.1->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.7)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain-community) (2.23.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.6/2.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.4 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.1 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fe98be-128d-4fac-a6c4-a56e0d58160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7f1a4e-6b31-4c83-845f-b231f7948462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f6f092-fb33-4aec-a333-00682cff9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"data\", glob = \"./*.txt\", loader_cls= TextLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19f1bc2-3edf-44c1-a81c-d80f78c99c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\dl.txt'}, page_content='The specific kind of neural networks used for LLMs are called transformer models. Transformer models are able to learn context â€” especially important for human language, which is highly context-dependent. Transformer models use a mathematical technique called self-attention to detect subtle ways that elements in a sequence relate to each other. This makes them better at understanding context than other types of machine learning. It enables them to understand, for instance, how the end of a sentence connects to the beginning, and how the sentences in a paragraph relate to each other.\\n\\nThis enables LLMs to interpret human language, even when that language is vague or poorly defined, arranged in combinations they have not encountered before, or contextualized in new ways. On some level they \"understand\" semantics in that they can associate words and concepts by their meaning, having seen them grouped together in that way millions or billions of times.'),\n",
       " Document(metadata={'source': 'data\\\\gen.txt'}, page_content='Generative AI models are growing in popularity, since they offer a number of potential benefits. These benefits include, but are not limited to:\\n\\nContent ideation: The use of generative AI can help content creators come up with a creative direction more quickly.\\nBetter chatbots: Generative AI models can be integrated into chatbots to better answer customer questions, engage prospects, and so on.\\nEnhanced research: Generative AI models can rapidly process vast amounts of data, including medical data or scientific studies, to aid in research.\\nImproved search results: Search engines and virtual assistants can incorporate generative AI capabilities to more quickly provide relevant information in response to queries.\\nEntertainment: Many people use publicly available generative AI tools solely for fun.\\nOther benefits: AI is a rapidly growing field, and further benefits from generative AI are likely still yet to come.\\nHowever, generative AI does come with its share of drawbacks, including:\\n\\nHallucination and other inaccuracies: Generative AI models are typically very good at identifying patterns, but sometimes they identify patterns that do not actually exist. This can result in the models providing false information, a phenomenon known as \"hallucination.\" Additionally, generative AI models are only as accurate as the data they are fed, and fact-checking generative AI outputs can prove difficult without access to the source data.\\nData leaks: Models can take the data they are fed in prompts and reveal it in unexpected contexts. Several large enterprises have accidentally leaked confidential information or source code in this way.\\nAccidental plagiarism or misuse of intellectual property: Because generative AI models are based on preexisting content, they may reproduce content they were fed without permission from that content\\'s original author or copyright holder.\\nMalicious response manipulation: Attackers can feed data to a generative AI model that causes it to produce dangerous or unsafe information for other users.\\nBiases: Any biases in the information fed to a model as it is trained are likely to be retained or even exacerbated unless the model is fine-tuned to correct for them. And even then, ensuring results are free from bias without reviewing the entire training set is near-impossible.'),\n",
       " Document(metadata={'source': 'data\\\\llm.txt'}, page_content='A key characteristic of LLMs is their ability to respond to unpredictable queries. A traditional computer program receives commands in its accepted syntax, or from a certain set of inputs from the user. A video game has a finite set of buttons, an application has a finite set of things a user can click or type, and a programming language is composed of precise if/then statements.\\n\\nBy contrast, an LLM can respond to natural human language and use data analysis to answer an unstructured question or prompt in a way that makes sense. Whereas a typical computer program would not recognize a prompt like \"What are the four greatest funk bands in history?\", an LLM might reply with a list of four such bands, and a reasonably cogent defense of why they are the best.\\n\\nIn terms of the information they provide, however, LLMs can only be as reliable as the data they ingest. If fed false information, they will give false information in response to user queries. LLMs also sometimes \"hallucinate\": they create fake information when they are unable to produce an accurate answer. For example, in 2022 news outlet Fast Company asked ChatGPT about the company Tesla\\'s previous financial quarter; while ChatGPT provided a coherent news article in response, much of the information within was invented.\\n\\nIn terms of security, user-facing applications based on LLMs are as prone to bugs as any other application. LLMs can also be manipulated via malicious inputs to provide certain types of responses over others â€” including responses that are dangerous or unethical. Finally, one of the security problems with LLMs is that users may upload secure, confidential data into them in order to increase their own productivity. But LLMs use the inputs they receive to further train their models, and they are not designed to be secure vaults; they may expose confidential data in response to queries from other users.'),\n",
       " Document(metadata={'source': 'data\\\\ml.txt'}, page_content='At a basic level, LLMs are built on machine learning. Machine learning is a subset of AI, and it refers to the practice of feeding a program large amounts of data in order to train the program how to identify features of that data without human intervention.\\n\\nLLMs use a type of machine learning called deep learning. Deep learning models can essentially train themselves to recognize distinctions without human intervention, although some human fine-tuning is typically necessary.\\n\\nDeep learning uses probability in order to \"learn.\" For instance, in the sentence \"The quick brown fox jumped over the lazy dog,\" the letters \"e\" and \"o\" are the most common, appearing four times each. From this, a deep learning model could conclude (correctly) that these characters are among the most likely to appear in English-language text.\\n\\nRealistically, a deep learning model cannot actually conclude anything from a single sentence. But after analyzing trillions of sentences, it could learn enough to predict how to logically finish an incomplete sentence, or even generate its own sentences.'),\n",
       " Document(metadata={'source': 'data\\\\que.txt'}, page_content='What is a machine learning model?\\nAn algorithm is a set of preprogrammed steps; a machine learning model is the result when an algorithm is applied to a collection of data. Despite this distinction, the terms \"machine learning model\" and \"machine learning algorithm\" are sometimes used interchangeably. But the difference is important: two machine learning models can produce different results even if they use the same algorithm, as long as each model has been fed different data as a starting point.\\n\\nWhat is deep learning?\\nDeep learning is a type of machine learning. It uses neural networks in order to learn to recognize patterns and make associations in raw, unstructured data. Deep learning is unsupervised and can perform extremely complex tasks. It is often used for speech recognition, automated driving, and other advanced applications.\\n\\nWhat is a neural network?\\nA neural network is a method of machine learning that imitates the structure of the human brain. Neural networks are comprised of nodes that connect to each other. These nodes are spread across at least three layers: an input layer, an output layer, and one or more hidden layers.\\n\\nEach layer contains several nodes that connect to each other. If a node perceives data as significant, it passes that data to the next node.\\n\\nThink back to the chef making dishes in the kitchen:\\n\\nIf the chef has to make a cake, they might start by examining the ingredients in the pantry; this is like the input layer of a neural network.\\nThe chef selects ingredients like flour, eggs, sugar, and cocoa powder. Ingredients like chicken stock or rice, meanwhile, are not selected. This is like passing statistically significant data on to the next node.\\nThe chef combines ingredients in various ways â€” mixing cake batter, making frosting, and so on. Think of this as the hidden layers of a neural network, with nodes passing data to each other.\\nFinally, the chef bakes, frosts, and serves the cake; this is like the output layer. Along the way, irrelevant or incorrect data (like unnecessary ingredients and incorrectly mixed combinations) was eliminated.\\nWhat is a vector database?\\nA vector database is a method for storing data that enhances machine learning. Vector databases allow for similarity searches and identifying related items, as opposed to exact match queries. Storing data in this way helps machine learning models understand the context for the inputs they receive.\\n\\nA vector database stores items in a matrix with various dimensions, and with vectors specifying each item of data\\'s position along those dimensions. This allows machine learning models to find data in relation to other data. For example, a streaming platform can pair machine learning with a vector database in order to identify which movies to recommend to a viewer, based on their past viewing history.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedb9eb7-5f5e-4a25-977c-b7c3bf9bfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d23f8d1e-9f71-4302-861a-630fd83827c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface_hub)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: filelock, huggingface_hub\n",
      "Successfully installed filelock-3.16.1 huggingface_hub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires pandas, which is not installed.\n",
      "datasets 2.20.0 requires pyarrow>=15.0.0, which is not installed.\n",
      "datasets 2.20.0 requires pyarrow-hotfix, which is not installed.\n",
      "datasets 2.20.0 requires xxhash, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f8ce1d-37b3-42f6-b226-7ce0df1302f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8340\\36401353.py:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\Lang\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "huggingfacehub_api_token = 'key'\n",
    "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06abcdc5-a9f1-4c0c-ba87-c74fd7c2eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers) (4.45.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers) (0.25.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42997f0c-1a9f-45c9-8781-1c01ac8a0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47311181-3fcb-492d-bb33-5483d78ae77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "text = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215d26f-abb7-43f3-96d6-6a9cfefbf540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ad17d-296e-480a-b186-b3d4c39e86ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c0a798-f020-4e24-b84d-0c98c65f3c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.6 MB 2.1 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.0/11.6 MB 2.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 2.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/11.6 MB 2.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.6/11.6 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.7/11.6 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.9/11.6 MB 2.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.5/11.6 MB 2.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.0/11.6 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.8/11.6 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.0/11.6 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.3/11.6 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.1/11.6 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires pyarrow>=15.0.0, which is not installed.\n",
      "datasets 2.20.0 requires pyarrow-hotfix, which is not installed.\n",
      "datasets 2.20.0 requires xxhash, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5318f1ee-3240-4ba6-aee0-25cdd3172b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.2 MB 1.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.8/25.2 MB 1.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.8/25.2 MB 1.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 1.0/25.2 MB 1.2 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 1.3/25.2 MB 1.1 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 1.6/25.2 MB 1.1 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.8/25.2 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 2.1/25.2 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 2.4/25.2 MB 1.2 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 2.9/25.2 MB 1.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.1/25.2 MB 1.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 3.4/25.2 MB 1.3 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 3.9/25.2 MB 1.4 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 4.5/25.2 MB 1.5 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 5.0/25.2 MB 1.5 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 5.5/25.2 MB 1.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 5.8/25.2 MB 1.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 6.3/25.2 MB 1.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 6.8/25.2 MB 1.7 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 7.6/25.2 MB 1.7 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 7.9/25.2 MB 1.7 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 8.1/25.2 MB 1.7 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 8.4/25.2 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 8.9/25.2 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 9.2/25.2 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 9.4/25.2 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 10.0/25.2 MB 1.7 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 1.7 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 10.7/25.2 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 11.5/25.2 MB 1.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 12.1/25.2 MB 1.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 12.6/25.2 MB 1.8 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 13.1/25.2 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 13.6/25.2 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 14.4/25.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/25.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 15.2/25.2 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 15.7/25.2 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 16.3/25.2 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.8/25.2 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 17.3/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.6/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 19.1/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 19.7/25.2 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/25.2 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.7/25.2 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 21.2/25.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/25.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.5/25.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.3/25.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.6/25.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-17.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires pyarrow-hotfix, which is not installed.\n",
      "datasets 2.20.0 requires xxhash, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c9cdb9e-4c28-4b76-902f-1aac71e9d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow_hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow_hotfix\n",
      "Successfully installed pyarrow_hotfix-0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires xxhash, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow_hotfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "503ab4bb-cfda-436e-852c-5207b5e8e2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash\n",
      "Successfully installed xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xxhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1460dff9-c990-4ecf-a342-15f02ea8bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "persist_directory = 'db2'\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=text,\n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04dd5b52-140e-4fa2-afb3-8549735cd8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Using cached chromadb-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (4.11.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.19.2-cp311-cp311-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (0.20.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (1.66.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (4.2.0)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (3.10.7)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.38.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.7)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (75.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.25.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Using cached chromadb-0.5.11-py3-none-any.whl (603 kB)\n",
      "Using cached build-1.2.2-py3-none-any.whl (22 kB)\n",
      "Using cached fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached onnxruntime-1.19.2-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "Using cached opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Using cached opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Using cached posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
      "Using cached rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Using cached importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: opentelemetry-proto, markdown-it-py, importlib-metadata, humanfriendly, googleapis-common-protos, deprecated, build, rich, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, google-auth, coloredlogs, typer, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "Successfully installed build-1.2.2 chromadb-0.5.11 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.115.0 google-auth-2.35.0 googleapis-common-protos-1.65.0 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-31.0.0 markdown-it-py-3.0.0 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 posthog-3.6.6 rich-13.8.1 typer-0.12.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc3e8d99-ca78-474c-8992-6ea8c53cd4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-huggingface) (0.25.1)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-huggingface) (0.3.6)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-huggingface) (3.1.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-huggingface) (0.20.0)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-huggingface) (4.45.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.1.129)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\lang\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e2c96-d9f7-49dc-a8de-4944cb74c1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39a592d7-31d6-4f4a-8cac-ae4dd37622f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8340\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "186d8477-9a97-4f40-adfe-b0da124df413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8340\\2004002305.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=persist_directory,\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab0590a-e3ed-4741-9479-888ed7b50875",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a9020-3f14-43a1-91b0-f6b6a10f9620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2479a2b7-9c13-4835-9964-a99127d444da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8340\\3873752345.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"what is llm?\")\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"what is llm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6110ef2-9050-444f-9f24-07b0732e521c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\ml.txt'}, page_content='At a basic level, LLMs are built on machine learning. Machine learning is a subset of AI, and it refers to the practice of feeding a program large amounts of data in order to train the program how to identify features of that data without human intervention.\\n\\nLLMs use a type of machine learning called deep learning. Deep learning models can essentially train themselves to recognize distinctions without human intervention, although some human fine-tuning is typically necessary.'),\n",
       " Document(metadata={'source': 'data\\\\llm.txt'}, page_content='own productivity. But LLMs use the inputs they receive to further train their models, and they are not designed to be secure vaults; they may expose confidential data in response to queries from other users.'),\n",
       " Document(metadata={'source': 'data\\\\llm.txt'}, page_content='By contrast, an LLM can respond to natural human language and use data analysis to answer an unstructured question or prompt in a way that makes sense. Whereas a typical computer program would not recognize a prompt like \"What are the four greatest funk bands in history?\", an LLM might reply with a list of four such bands, and a reasonably cogent defense of why they are the best.'),\n",
       " Document(metadata={'source': 'data\\\\llm.txt'}, page_content='A key characteristic of LLMs is their ability to respond to unpredictable queries. A traditional computer program receives commands in its accepted syntax, or from a certain set of inputs from the user. A video game has a finite set of buttons, an application has a finite set of things a user can click or type, and a programming language is composed of precise if/then statements.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21065ff6-853b-4a17-b536-299be3a8fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8531c560-5fea-4fbf-999b-e4560e384180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce8e51d-6cb8-4cee-b2ad-5c3c919f45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1faa973-d449-474b-be59-620454216b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "090cf22e-7e95-47d7-a56c-d5d33448d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatt(response):\n",
    "    print(response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77f94cb1-64e3-4a81-be97-6c52552f0d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "At a basic level, LLMs are built on machine learning. Machine learning is a subset of AI, and it refers to the practice of feeding a program large amounts of data in order to train the program how to identify features of that data without human intervention.\n",
      "\n",
      "LLMs use a type of machine learning called deep learning. Deep learning models can essentially train themselves to recognize distinctions without human intervention, although some human fine-tuning is typically necessary.\n",
      "\n",
      "own productivity. But LLMs use the inputs they receive to further train their models, and they are not designed to be secure vaults; they may expose confidential data in response to queries from other users.\n",
      "\n",
      "Question: What is llm?\n",
      "Helpful Answer:\n",
      "\n",
      "LLMs are built on machine learning, which is a subset of AI. They use deep learning models to recognize distinctions without human intervention, although some human fine-tuning is typically necessary. LLMs can be used for various purposes, such as analyzing text or images, but they may expose confidential data in response to queries from other users.\n",
      "\n",
      "\n",
      "Sources:\n",
      "data\\ml.txt\n",
      "data\\llm.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"What is llm?\"\n",
    "response = qa_chain(query)\n",
    "formatt(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6f69531-f9e0-4676-8a18-8546c5a30c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Content ideation: The use of generative AI can help content creators come up with a creative direction more quickly.\n",
      "Better chatbots: Generative AI models can be integrated into chatbots to better answer customer questions, engage prospects, and so on.\n",
      "Enhanced research: Generative AI models can rapidly process vast amounts of data, including medical data or scientific studies, to aid in research.\n",
      "\n",
      "Generative AI models are growing in popularity, since they offer a number of potential benefits. These benefits include, but are not limited to:\n",
      "\n",
      "Question: What is Generative AI?\n",
      "Helpful Answer: Generative AI is a type of AI that uses machine learning and natural language processing to create something new, such as a creative direction, chatbot, or research output.\n",
      "\n",
      "\n",
      "Sources:\n",
      "data\\gen.txt\n",
      "data\\gen.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Generative AI?\"\n",
    "response = qa_chain(query)\n",
    "formatt(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914c58e-c340-483e-bf44-d96ee63ddb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
